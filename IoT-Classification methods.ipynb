{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AFI Escuela de finanzas**\n",
    "\n",
    "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRsvzArVKQ5jGTVEqwdNneQFIgYVvjLPbYNvxAfFV_iktBaf9u&s)\n",
    "\n",
    "## **MÃ¡ster Executive en Data Science y Big Data en Finanzas**\n",
    "\n",
    "**17 de Enero de 2020**\n",
    "\n",
    "# **IoT Use cases**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical Session : Classification methods**\n",
    "\n",
    "Starting from a dataset of water consumption that you can find in dataset_eventos.csv.\n",
    "\n",
    "\n",
    "In this lab session we are going to deep in our knowledge about classifiers by managing most well-known classification algorithms. Besides, we are going to review some useful techniques, such as the cross validation process, which will allow us to adjust the free parameters of the classifier. \n",
    "\n",
    "#### ** During this lab we will cover: **\n",
    "\n",
    "#### * Part 1: Linear models*\n",
    "#### * Part 2: K-Nearest Neighbours (K-NN)*\n",
    "#### * Part 3: Support Vector Machines (SVMs) with different kernel funcions*\n",
    "#### * Part 4: Tree based algorithms*\n",
    "#### * Part 5: Neural Networks*\n",
    "\n",
    "\n",
    "As in previous lab session, to implement the different approaches we will base in [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ** Part 0: Load and prepare the data **\n",
    "\n",
    "Thidataset consists of 6 classes of water consumption (tap, toilet, shower,...)\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training and test data partitions, as well as normalize them.\n",
    "\n",
    "Useful functions: [make_classification( )](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html), [train_test_split( )](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) and [StandardScaler( )](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "def ReadEvents(file):\n",
    "    data = np.loadtxt(file,skiprows=1,delimiter=';',usecols=range(0,37))\n",
    "    labels = np.loadtxt(file,skiprows=1,delimiter=';',usecols=(37,),dtype='str')\n",
    "    (nSamples,nFeatures)=data.shape\n",
    "    randomPermutation = np.random.permutation(nSamples)\n",
    "    data=data[randomPermutation,:]\n",
    "    labels=labels[randomPermutation]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.unique(labels))\n",
    "    labels = le.transform(labels)\n",
    "    return data,labels\n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print('The first time that you downlaod the data it can take a while...')\n",
    "import numpy as np\n",
    "#dataset = np.loadtxt('./dataPrepared.csv', delimiter=',',skiprows=1)\n",
    "#X = dataset[:,:-1]\n",
    "#Y=dataset[:,-1]\n",
    "X,Y = ReadEvents('./dataset_eventos.csv')\n",
    "\n",
    "# for machine learning we use the data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Binarize the labels for some feature selection methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Linear models**\n",
    "\n",
    "Include the necessary code to train and test a classifier based in:\n",
    "1. A logistic regression model: in thiscase adjust the C parameter by CV\n",
    "2. Linear Discrimation Analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rang_C = np.logspace(-3, 3, 10)\n",
    "tuned_parameters = [{'C': rang_C}]\n",
    "nfold = 10\n",
    "\n",
    "# Train a LR model and adjust by CV the parameter C\n",
    "clf_LR  = # <FILL IN> \n",
    "acc_test_LR=# <FILL IN> \n",
    "\n",
    "# LDA \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "clf_LDA = LDA()\n",
    "clf_LDA.# <FILL IN> \n",
    "acc_test_LDA=# <FILL IN> \n",
    "\n",
    "print(\"The test accuracy of LR is %2.2f\" %(100*acc_test_LR))\n",
    "print(\"The test accuracy of LDA is %2.2f\" %(100*acc_test_LDA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 2: K nearest neigbors**\n",
    "\n",
    "A K-NN approach classifies each new data searching its K nearest neighbors (among the training data) and assigning the majority class among these neighbors. As expected, its performance depends on the number of neighbors (K) used.\n",
    "\n",
    "To start to work, let's analyze for different values of K the K-NN performance, both over training and test sets. Use the [KNeighborsClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) method to complete the below code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure points out the necessity of selecting the adequate value of K. And, as expected, using the training error for such selection would provide a poor generalization.\n",
    "\n",
    "#### ** Selecting the number of neighbors of a K-NN classifier**\n",
    "\n",
    "Therefore, next step will consist of applying a cross validation (CV) process to select the optimum value of K. You can use the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function to implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters\n",
    "K_max = 50\n",
    "rang_K = np.arange(1, K_max+1)\n",
    "nfold = 10\n",
    "# Define a dictionary with the name of the parameters to explore as a key and the ranges to explores as value\n",
    "tuned_parameters = [{'n_neighbors': rang_K}]\n",
    "\n",
    "\n",
    "# Cross validation proccess \n",
    "clf_base = neighbors.KNeighborsClassifier( )\n",
    "# Define the classfifier with the CV process (use GridSearchCV here!!!)\n",
    "clf =  #<FILL IN>\n",
    "# Train it (this executes the CV)\n",
    "clf.#<FILL IN>\n",
    "\n",
    "print('CV process sucessfully finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the CV process, the classifier object  contains the information of the CV process (next cell explore the parameter \".grid\\_scores\\_\" to obtain this information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing results\n",
    "print(\"Cross validation results:\")\n",
    "\n",
    "paramsFolds = clf.cv_results_['params']\n",
    "meanScoreFolds = clf.cv_results_['mean_test_score']\n",
    "stdScoreFolds = clf.cv_results_['std_test_score']\n",
    "\n",
    "for fold in range(K_max):\n",
    "    params = paramsFolds[fold]\n",
    "    mean_score = meanScoreFolds[fold]\n",
    "    std_score = stdScoreFolds[fold]\n",
    "    print(\"For K = %d, validation accuracy is %2.2f (+/-%1.3f)%%\" \n",
    "          % (params['n_neighbors'], 100*mean_score, 100*std_score / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the fields \".best\\_estimator\\_\" and \".best\\_params\\_\" of the classifier generated by the CV process:\n",
    "* \".best\\_estimator\\_\" contains  the final classifier trained with this select value.\n",
    "* \".best\\_params\\_\" is a dictionary with the selected parameters. In our example, \"best\\_params\\_['n\\_neighbors']\" would provide the selected value of K.\n",
    "\n",
    "Save the selected value of K in variable denoted \"K_opt\" and compute the test error of the final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# Assign to K_opt the value of K selected by CV\n",
    "K_opt = # <FILL IN>\n",
    "print(\"The value optimum of K is %d\" %(K_opt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also compute the test error directly over the classifier object return by the CV process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_acc_test = clf.score(X_test, Y_test)\n",
    "print(\"The test accuracy is %2.2f\" %(100*KNN_acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: SVM**\n",
    "\n",
    "SVM is one of the most well-known classifiers due to its good generalization properties in many different applications. Besides, by means of the kernel trick, its linear formulation can easily extended to a non linear fashion. \n",
    "\n",
    "Here, we will test its performance when different kernel functions are used. For this purpose, we can use the [SCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) method, which let you select the kernel function to be used, and the method GridSearchCV( ) to adjust the different free parameters (C and kernel parameter). \n",
    "\n",
    "Complete the following cells, when it is required, to train in each case a linear SVM (defining kernel='linear' in the method SCV( )), an SVM with gaussian kernel (kernel='rbf') and an SVM with polynomial kernel (kernel='poly'). \n",
    "\n",
    "For each method, adjust the corresponding free parameters with a 10 fold CV process (the ranges to explore are defined at the beginning of each cell). Return the values of selected parameters and the accuracy of the final SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** SVM with gaussian kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn import svm\n",
    "n_dim=X_train.shape[1]\n",
    "rang_C=np.array([10, 100])\n",
    "rang_gamma=np.array([10, 100])\n",
    "tuned_parameters = [{'C': rang_C, 'gamma': rang_g}]\n",
    "\n",
    "# Train an SVM with gaussian kernel and adjust by CV the parameter C\n",
    "clf_base = svm.SVC(kernel='rbf')\n",
    "selection = np.array([2,9,0,6,4,10,1,7,3])\n",
    "rbf_svc  =  # <FILL IN> \n",
    "rbf_svc. # <FILL IN> \n",
    "# Save the values of C and gamma selected and compute the final accuracy\n",
    "C_opt = # <FILL IN> \n",
    "g_opt = # <FILL IN> \n",
    "\n",
    "\n",
    "print(\"The C value selected is \" + str(C_opt))\n",
    "print(\"The gamma value selected is \" + str(g_opt))\n",
    "acc_rbf_svc = rbf_svc.score(X_test[:,selection], Y_test)\n",
    "print(\"The test accuracy of the RBF SVM is %2.2f\" %(100*acc_rbf_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** SVM with polynomial kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "rang_d=np.arange(1,5)\n",
    "tuned_parameters = [{'C': rang_C, 'degree': rang_d}]\n",
    "\n",
    "# Train an SVM with polynomial kernel and adjust by CV the parameter C\n",
    "clf_base =  svm.SVC(kernel='poly')\n",
    "\n",
    "poly_svc  = # <FILL IN> \n",
    "poly_svc.# <FILL IN> \n",
    "\n",
    "# Save the values of C and degree selected and compute the final accuracy\n",
    "C_opt = # <FILL IN> \n",
    "d_opt = # <FILL IN> \n",
    "\n",
    "\n",
    "print(\"The C value selected is \" + str(C_opt))\n",
    "print(\"The degree value selected is \" + str(d_opt))\n",
    "acc_poly_svc = poly_svc.score(X_test, Y_test)\n",
    "print(\"The test accuracy of the polynomial SVM is %2.2f\" %(100*acc_poly_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 4: Trees**\n",
    "\n",
    "** Training a Random Forest**\n",
    "A Random Forest (RF) trains several decision tree classifiers, where each one is trained with different samples and features of the training data, and averages their outputs to improve the final accuracy.\n",
    "\n",
    "Use the RandomForestClassifier( ) function to train a RF classifier and select by cross validation the number of trees to use. The remaining parameters, such as the number of subsampled data or features, can be used with their default values. Return the optimal number of trees to be used and the final accuracy of the RF classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rang_n_trees=np.arange(1,10)\n",
    "tuned_parameters = [{'n_estimators': rang_n_trees}]\n",
    "nfold = 10\n",
    "\n",
    "clf_RF  = #<FILL IN>\n",
    "clf_RF.fit(X_train, Y_train)\n",
    "n_trees_opt = #<FILL IN>\n",
    "acc_RF = #<FILL IN>\n",
    "\n",
    "print(\"The number of selected trees is \" + str(n_trees_opt))\n",
    "print(\"The test accuracy of the RF is %2.2f\" %(100*acc_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 5: Neural Networks**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "\n",
    "numlabels = len(np.unique(Y_train))\n",
    "labels = np.eye(numlabels)[np.int_(Y_train)]\n",
    "n_epoch = 20\n",
    "batch_size = 16\n",
    "hidden=100\n",
    "\n",
    "Learning_rate=0.01\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hidden,input_dim=X_train.shape[1],init='lecun_uniform',activation='relu'))\n",
    "model.add(Dense(output_dim=6,init='lecun_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "decay = Learning_rate/n_epoch\n",
    "nadam = Nadam(lr=Learning_rate)\n",
    "            \n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "model.fit(X_train, labels, nb_epoch=n_epoch, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "scores = model.evaluate(X_test,  np.eye(numlabels)[np.int_(Y_test)], verbose=0)\n",
    "print(model.metrics_names)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
