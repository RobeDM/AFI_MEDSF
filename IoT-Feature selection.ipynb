{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AFI Escuela de finanzas**\n",
    "\n",
    "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTRsvzArVKQ5jGTVEqwdNneQFIgYVvjLPbYNvxAfFV_iktBaf9u&s)\n",
    "\n",
    "## **MÃ¡ster Executive en Data Science y Big Data en Finanzas**\n",
    "\n",
    "**17 de Enero de 2019**\n",
    "\n",
    "# **IoT Use cases**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical Session : Feature selection**\n",
    "\n",
    "Starting from a dataset of water consumption that you can find in dataset_eventos.csv.\n",
    "\n",
    "\n",
    "In this session we are going work with some methods for feature selection.\n",
    "\n",
    "#### ** We will cover: **\n",
    "#### *Part 1: Filter methods for feature selection* \n",
    "##### *   Part 1.1: F-score* \n",
    "##### *   Part 1.2: Mutual Information* \n",
    "##### *   Part 1.3: Random Forest* \n",
    "#### *Part 2: Recursive Feature Elimination*\n",
    "#### *Part 3: Embedded methods. L1-regularization* \n",
    "\n",
    "To implement the different approaches we will base on [Scikit-Learn](http://scikit-learn.org/stable/) python toolbox.\n",
    "\n",
    "As you progress in this notebook, you will have to complete some exercises. Each exercise includes an explanation of what is expected, followed by code cells where one or several lines will have written down `<FILL IN>`.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line.  Once the `<FILL IN>` sections are updated and the code can be run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 0: Download and prepare the data **\n",
    "\n",
    "Olivetti dataset consists of ten different images of each of 40 distinct subjects. For some subjects, the images were taken at different times, varying the lighting, facial expressions (open / closed eyes, smiling / not smiling) and facial details (glasses / no glasses). All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).\n",
    "    \n",
    "The next code includes the lines to download this data set and create the training, validation and test data partitions, as well as normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "def ReadEvents(file):\n",
    "    data = np.loadtxt(file,skiprows=1,delimiter=';',usecols=range(0,37))\n",
    "    labels = np.loadtxt(file,skiprows=1,delimiter=';',usecols=(37,),dtype='str')\n",
    "    (nSamples,nFeatures)=data.shape\n",
    "    randomPermutation = np.random.permutation(nSamples)\n",
    "    data=data[randomPermutation,:]\n",
    "    labels=labels[randomPermutation]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.unique(labels))\n",
    "    labels = le.transform(labels)\n",
    "    return data,labels\n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "print('The first time that you downlaod the data it can take a while...')\n",
    "import numpy as np\n",
    "#dataset = np.loadtxt('./dataPrepared.csv', delimiter=',',skiprows=1)\n",
    "#X = dataset[:,:-1]\n",
    "#Y=dataset[:,-1]\n",
    "X,Y = ReadEvents('./dataset_eventos.csv')\n",
    "\n",
    "# for machine learning we use the data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "n_classes = np.unique(Y).shape[0]\n",
    "\n",
    "print(\"Dataset size information:\")\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Preparing the data\n",
    "\n",
    "# Initialize the random generator seed to compare results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.75)\n",
    "\n",
    "# split into a training and validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.333)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Binarize the labels for some feature selection methods\n",
    "set_classes = np.unique(Y)\n",
    "Y_train_bin = label_binarize(Y_train, classes=set_classes)\n",
    "\n",
    "print(\"Number of training samples: %d\" % X_train.shape[0])\n",
    "print(\"Number of validation samples: %d\" % X_val.shape[0])\n",
    "print(\"Number of test samples: %d\" % X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 1: Filtering methods **\n",
    "\n",
    "Filtering methods are characterized for being independent from the subsequent classification stage, so they use a relevance criterion to measure the discriminatory capability of each feature and select or rank the input features according this criterion.\n",
    "\n",
    "Here, we are going to study two possible approaches: \n",
    "* F-score (the F-test reduces to two-sample T-test when we work with binary problems): which uses a statistical test to evaluate whether the data of the different classes have been generated by different distributions or not. In this case, this criterion evaluates the relevance independently for each feature, so it is said to be univariate.\n",
    "* Random Forest: in this case we can train a random forest and analyze the number of times that a feature have been used in the forest. More used features will be more relevant. In this case the relevance of a feature is analyzed in combination with the remaining ones (each tree of the forest use several features and the fact that one is selected depends on the those selected previously), so this feature selection criterion is call multivariate.\n",
    "\n",
    "After selecting the subset of relevant features, we will analyze their discriminatory capability using a linear SVM as classifier and use its final test accuracy to evaluate the goodness of the different selection methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.1: F-score **\n",
    "\n",
    "Use the [f_classif()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html) method to obtain the F-score of each input feature. Use the resulting values to rank the features by relevance (starting by the most relevant) and provide in ind\\_rel\\_feat the position of the sorted features (i.e. ind_rel_feat[0] has contain the position of the most relevant feature). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = make_pipeline(LinearSVC())\n",
    "clf.fit(X_train, Y_train)\n",
    "print('Classification accuracy without selecting features: {:.3f}'\n",
    "      .format(clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "# F-score\n",
    "from sklearn.feature_selection import f_classif\n",
    "F, p = f_classif(#<FILL IN>)  \n",
    "F, p = f_classif(X_train,Y_train) \n",
    "\n",
    "# sort in descending order\n",
    "ind_rel_feat = # <FILL IN> \n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], F[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(LinearSVC())\n",
    "clf.fit(X_train[:,ind_rel_feat[0:20]], Y_train)\n",
    "print('Classification accuracy with selecting features: {:.3f}'\n",
    "      .format(clf.score(X_test[:,ind_rel_feat[0:20]], Y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to provide you two functions:\n",
    "* SVM_accuracy_evolution( ) which analyzes the accuracy of a linear SVM when different number of features are used, specifically, those given by the input variable rang\\_feat. Besides, it computes the training, validation and test accuracy, so we can later use these values to select the optimum number features and select the subset of relevant ones.\n",
    "* plot_accuracy_evolution( ) which directly let you plot the results provided by  SVM_accuracy_evolution( ).\n",
    "\n",
    "Note that you must provide to SVM_accuracy_evolution() the data with the variables sorted by relevance, so that the most relevant ones are used first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def SVM_accuracy_evolution(X_train_s, Y_train, X_val_s, Y_val, X_test_s, Y_test, rang_feat):\n",
    "    \"\"\"Compute the accuracy of training, validation and test data for different the number of features given\n",
    "        in rang_feat.\n",
    "\n",
    "    Args:\n",
    "        X_train_s (numpy dnarray): training data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_train (numpy dnarray): labels of the training data (number data x 1).\n",
    "        X_val_s (numpy dnarray): validation data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_val (numpy dnarray): labels of the validation data (number data x 1).\n",
    "        X_test_s (numpy dnarray): test data sorted by relevance (more relevant are first) (number data x number dimensions).\n",
    "        Y_test (numpy dnarray): labels of the test data (number data x 1).\n",
    "        rang_feat: range with different number of features to be evaluated                                           \n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the model to train a liner SVM and adjust by CV the parameter C\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    acc_tr = []\n",
    "    acc_val = []\n",
    "    acc_test = []\n",
    "    for i in rang_feat:\n",
    "        # Train SVM classifier\n",
    "        clf.fit(X_train_s[:, :i], Y_train)\n",
    "        # Compute accuracies\n",
    "        acc_tr.append(clf.score(X_train_s[:, :i], Y_train))\n",
    "        acc_val.append(clf.score(X_val_s[:, :i], Y_val))\n",
    "        acc_test.append(clf.score(X_test_s[:, :i], Y_test))\n",
    "\n",
    "    return np.array(acc_tr), np.array(acc_val), np.array(acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test):\n",
    "\n",
    "    \"\"\"Plot the accuracy evolution for training, validation and test data sets.\n",
    "    Args:\n",
    "        rang_feat: range with different number of features where the accuracy has been evaluated   \n",
    "        acc_tr: numpy vector with the training accuracies\n",
    "        acc_val: numpy vector with the validation accuracies\n",
    "        acc_test: numpy vector with the test accuracies                                          \n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(rang_feat, acc_tr, \"b\", label=\"train\")\n",
    "    plt.plot(rang_feat, acc_val, \"g\", label=\"validation\")\n",
    "    plt.plot(rang_feat, acc_test, \"r\", label=\"test\")\n",
    "    plt.xlabel(\"Number of features\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title('Accuracy evolution')\n",
    "    plt.legend(['Training', 'Validation', 'Test'], loc = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code lines to properly call to SVM\\_accuracy\\_evolution( ) function. Then, use the returned accuracies to validate the number of features to use and obtain its corresponding test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and using[:,ind_rel_feat]steps of 10 features)\n",
    "rang_feat = np.arange(1, 35, 1) \n",
    "\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print('Number optimum of features: ' + str(num_opt_feat))\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.2: Mutual Information **\n",
    "\n",
    "\n",
    "Compute the MI between each input feature and the output labes to measure the relevance of each feature. Check the function [mutual_info_classif()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif).\n",
    "\n",
    "Note: this MI estimator has a parameter (n_neighbors) which should be validated. You can use its default value for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Obtain MI values\n",
    "MI = mutual_info_classif(#<FILL IN>, random_state =0)  # Returns MI values\n",
    "    \n",
    "# sort in descending order\n",
    "ind_rel_feat = # <FILL IN>\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], MI[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(5, 1000, 10) \n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "print('Number optimum of features: ' + str(num_opt_feat))\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1.3: Random Forest **\n",
    "\n",
    "Train a random forest to obtain a measurement of the relevance of each feature. You can use the  [RandomForestClassifier()](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) function, which returns the feature relevances in the variable .feature\\_importances\\_.\n",
    "\n",
    "\n",
    "Complete the following code to:\n",
    "* Train a random forest classifier with the parameters given by default and 250 trees.\n",
    "* Use the forest feature importances to obtain a ranking with the most relevant features, save their positions in the variable  ind\\_rel\\_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "np.random.seed(1)\n",
    "# Build a forest and obtain the feature importances\n",
    "forest = RandomForestClassifier(n_estimators=250)\n",
    "forest.fit(X_train, Y_train)\n",
    "importances = # <FILL IN>\n",
    "\n",
    "# Obtain the positions of the sorted features (the most relevant first)\n",
    "ind_rel_feat =  # <FILL IN>\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, ind_rel_feat[f], importances[ind_rel_feat[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the above the result to:\n",
    "* Evaluate the linear SVM performance with the most relevant features are used (use SVM_accuracy_evolution( ) function). Plot the evolution of the training, validation and test accuracies.\n",
    "* Obtain, using the validation accuracy, the optimum number of features to use.\n",
    "* Create the mask with the selected features and plot it (you can use the plot_image( ) function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(1, 35, 1) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "\n",
    "print('Number optimum of features: ' + str(num_opt_feat))\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Wrapers for feature selection: RFE\n",
    "\n",
    "The Recursive Feature Elimination (RFE) method iteratively trains a set of SVM classifier and, in each step, it eliminates a feature (or a subset of features) in such a way that the classification margin is reduced the least.\n",
    "\n",
    "Scikit-Learn provides a function with a full implementation of the RFE method. This function, [RFE( )](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), let user select the classifier to consider, the number of selected features (parameter n\\_features\\_to\\_select) and the number of features removed in each step (parameter step).  As result, in parameter .ranking returns the ranking position of the each feature (i.e., .ranking\\_[i] corresponds to the ranking position of the i-th feature).\n",
    "\n",
    "Complete the following code to:\n",
    "* Train the RFE approach with a linear SVM fixing  n_features_to_select to 10 (so that we run the RFE method until the end obtaining a full raking with all the features) and step to 10 (just to speed up the training).\n",
    "* Use the content of .ranking\\_ to obtain a ranking with the most relevant features, save their positions in the variable  ind\\_rel\\_feat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print('The training of this method can take some minutes, be patient...')\n",
    "# Define the classifier, the RFE method and train it\n",
    "estimator = SVC(kernel=\"linear\")\n",
    "RFE_selector = RFE(estimator, 1, step=1)\n",
    "RFE_selector.fit(X_train, Y_train)\n",
    "\n",
    "# Obtain the positions of the sorted features (the most relevant first)\n",
    "ind_rel_feat = np.argsort(RFE_selector.ranking_)\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d\" % (f + 1, ind_rel_feat[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "# Define the range of features to explore\n",
    "# (explore the 1000 most relevant ones, starting with 5 and usings steps of 10 features)\n",
    "rang_feat = np.arange(1, 35, 1) # To speed up the execution, we use steps of 10 features.\n",
    "[acc_tr, acc_val, acc_test] = SVM_accuracy_evolution(#<FILL IN>)\n",
    "\n",
    "# Plot it!\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(rang_feat, acc_tr, acc_val, acc_test)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimum number of features\n",
    "num_opt_feat = #<FILL IN>\n",
    "test_acc_opt = #<FILL IN>\n",
    "\n",
    "\n",
    "print('Number optimum of features: ' + str(num_opt_feat))\n",
    "print(\"The optimum test accuracy is  %2.2f%%\" %(100*test_acc_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Part 3: Embedded methods:  L1 regularization**\n",
    "\n",
    "Embedded methods are able to carry out the feature selection process during the classifier training, so both stages (feature selection and classifier training) are completely linked and, therefore, the selection process is guided by the classifier.\n",
    "\n",
    "In this case, we are going to study two well-known techniques:  \n",
    "* L1 SVM: in this case we can train linear SVM regularized with a L1 penalty which is able to provide sparsity over the weight vector. As to obtain the classifier output we have to multiply the input data by this vector, those input features associated to the zeros of the weight vector are not used during the classification process (at least, in binary problem, we later analyze this in detail for the multiclass case). \n",
    "\n",
    "* L1 Logistic Regression: here, we will apply the above L1 penalty, but using a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3.1: L1-SVM**\n",
    "\n",
    "In this last section, we are going to analyze the properties of the L1-SVM as feature selection approach. For this purpose, we are going to use the linear SVM implementation given by the method LinearSVC, which directly let us select the regularization to be used.\n",
    "\n",
    "Unlike previous methods, here we cannot obtain a ranking of variables. In this case, we have to sweep the value of the regularization parameter in order to get a higher (or lower) sparsity of the weight vector. According to this, complete the following code to:\n",
    "* Train a linear l1-SVM for different values of the regularization parameter, for each value obtain the training, validation and test accuracies, as well as the sparsity rate. Sparsity Rate (SR) is defined as the mean number of zeros of the SVM weight vector (check the parameter .coef\\_ of the SVM classifier), then, if all elements are zero, SR has to be 100%.\n",
    "* Plot the resulting accuracy curves.\n",
    "* Finally, analyzing the validation accuracy, select the optimum value of C.\n",
    "\n",
    "Note:  As we are working with a multiclass problem (1 vs all scheme), we are going to obtain a set of vectors (one for each binary problem). We could define different approaches to work with this set of vectors, however, for the sake of simplicity, in this first exercise, let's compute the multiclass SR by averaging the SR of each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "###########################################################\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "np.random.seed(1)\n",
    "# Defining some useful variables to save results\n",
    "acc_tr = []\n",
    "acc_val = []\n",
    "acc_test = []\n",
    "sparsity_rate = []\n",
    "\n",
    "# Defining the range of C values to explore\n",
    "rang_C = 2*np.logspace(-2, 2, 20)\n",
    "\n",
    "print('The training of this method can take some minutes, be patient...')\n",
    "for i, C in enumerate(rang_C):\n",
    "    # Define and train SVM classifier\n",
    "    svm_l1 = LinearSVC(C=C, penalty=\"l1\", dual=False)\n",
    "    svm_l1.fit(X_train, Y_train)\n",
    "    \n",
    "    # Compute the sparsity rate (coef_l1 contains zeros due to the\n",
    "    # L1 sparsity inducing norm)\n",
    "    sparsity_rate.append((np.float_(np.size(svm_l1.coef_))-np.count_nonzero(svm_l1.coef_))/np.size(svm_l1.coef_))\n",
    "    \n",
    "    # Compute accuracies\n",
    "    acc_tr.append(svm_l1.score(X_train, Y_train))\n",
    "    acc_val.append(svm_l1.score(X_val, Y_val))\n",
    "    acc_test.append(svm_l1.score(X_test, Y_test))\n",
    "\n",
    "    \n",
    "# Plot the accuracy curves\n",
    "plt.figure()\n",
    "plot_accuracy_evolution(sparsity_rate, acc_tr, acc_val, acc_test)\n",
    "plt.xlabel(\"Sparsity rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
